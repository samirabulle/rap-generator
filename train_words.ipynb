{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PyLyrics import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import h5py as h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = open(\"lyrics.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samirabulle/Documents/projects/rap_generator/env/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/samirabulle/.pyenv/versions/3.6.4/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Song or Singer does not exist or the API does not have Lyrics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8409f1a20ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mall_lyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPyLyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'J. Cole'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkendrick_tracks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mall_lyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPyLyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kendrick Lamar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/projects/rap_generator/env/lib/python3.6/site-packages/PyLyrics/functions.py\u001b[0m in \u001b[0;36mgetLyrics\u001b[0;34m(singer, song)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mlyrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'lyricbox'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlyrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Song or Singer does not exist or the API does not have Lyrics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m#Remove Scripts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Song or Singer does not exist or the API does not have Lyrics"
     ]
    }
   ],
   "source": [
    "drake = \"\"\"Tuscan Leather\n",
    "Furthest Thing\n",
    "Started from the Bottom\n",
    "Wu-Tang Forever\n",
    "Own It\n",
    "Worst Behavior\n",
    "From Time (featuring JhenÃ© Aiko)\n",
    "Hold On, We're Going Home (featuring Majid Jordan)\n",
    "Connect\n",
    "The Language\n",
    "305 to My City (featuring Detail)\n",
    "Too Much (featuring Sampha)\n",
    "Pound Cake / Paris Morton Music 2 (featuring Jay-Z)\n",
    "Come Thru (Deluxe edition bonus track)\n",
    "All Me (featuring 2 Chainz and Big Sean) (Deluxe edition bonus track)\n",
    "The Motion (featuring Sampha) (Best Buy bonus track)\"\"\"\n",
    "\n",
    "j_cole = \"\"\"Intro\n",
    "January 28th\n",
    "Wet Dreamz\n",
    "03' Adolescence\n",
    "A Tale of 2 Citiez\n",
    "Fire Squad\n",
    "St. Tropez\n",
    "G.O.M.D.\n",
    "No Role Modelz\n",
    "Hello\n",
    "Apparently\n",
    "Love Yourz\n",
    "Note to Self\"\"\"\n",
    "\n",
    "kendrick = \"\"\"Sherane a.k.a. Master Splinter's Daughter\n",
    "Bitch, Don't Kill My Vibe\n",
    "Backseat Freestyle\n",
    "The Art of Peer Pressure\n",
    "Money Trees (featuring Jay Rock)\n",
    "Poetic Justice (featuring Drake)\n",
    "good kid\n",
    "m.A.A.d city (featuring MC Eiht)\n",
    "Swimming Pools (Drank) (Extended Version)\n",
    "Sing About Me, I'm Dying of Thirst\n",
    "Real (featuring Anna Wise)\n",
    "Compton (featuring Dr. Dre)\n",
    "The Recipe (featuring Dr. Dre) (Deluxe edition track)\n",
    "Black Boy Fly (Deluxe edition track)\n",
    "Now or Never (featuring Mary J. Blige) (Deluxe edition track)\n",
    "Collect Calls (featuring Kent Jamz) (iTunes deluxe edition track)\n",
    "Swimming Pools (Drank) (iTunes deluxe edition track)\n",
    "County Building Blues (Target bonus track)\"\"\"\n",
    "\n",
    "drake_tracks = drake.split(\"\\n\")\n",
    "j_cole_tracks = j_cole.split(\"\\n\")\n",
    "kendrick_tracks = kendrick.split(\"\\n\")\n",
    "\n",
    "for track in drake_tracks:\n",
    "    all_lyrics.write(PyLyrics.getLyrics('Drake',track) + '\\n')\n",
    "for track in j_cole_tracks:\n",
    "    all_lyrics.write(PyLyrics.getLyrics('J. Cole',track) + '\\n')\n",
    "for track in kendrick_tracks:\n",
    "    all_lyrics.write(PyLyrics.getLyrics('Kendrick Lamar',track) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics.close()\n",
    "raw_lyrics = open(\"lyrics.txt\",\"r\")\n",
    "raw_lyrics = raw_lyrics.read()\n",
    "raw_lyrics = raw_lyrics.lower()\n",
    "#lyric_tokens = word_tokenize(raw_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samirabulle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = sorted(list(set(lyric_tokens)))\n",
    "#word_to_int = dict((w,i) for i, w in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(lyric_tokens)\n",
    "n_vocab = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 9\n",
    "data_input = []\n",
    "data_output = []\n",
    "for i in range(0, n_words - seq_length, 1):\n",
    "    seq_in = lyric_tokens[i:i + seq_length]\n",
    "    seq_out = lyric_tokens[i + seq_length]\n",
    "    data_input.append([word_to_int[word] for word in seq_in])\n",
    "    data_output.append(word_to_int[seq_out])\n",
    "n_patterns = len(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = numpy.reshape(data_input, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data_in/float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = np_utils.to_categorical(data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(data_in.shape[1], data_in.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(data_out.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 6.2370\n",
      "Epoch 00001: loss improved from inf to 6.23678, saving model to weights-improvement-01-6.2368.hdf5\n",
      "37975/37975 [==============================] - 52s 1ms/step - loss: 6.2368\n",
      "Epoch 2/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 6.0192\n",
      "Epoch 00002: loss improved from 6.23678 to 6.01979, saving model to weights-improvement-02-6.0198.hdf5\n",
      "37975/37975 [==============================] - 53s 1ms/step - loss: 6.0198\n",
      "Epoch 3/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 6.0102\n",
      "Epoch 00003: loss improved from 6.01979 to 6.00987, saving model to weights-improvement-03-6.0099.hdf5\n",
      "37975/37975 [==============================] - 51s 1ms/step - loss: 6.0099\n",
      "Epoch 4/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 6.0047\n",
      "Epoch 00004: loss improved from 6.00987 to 6.00405, saving model to weights-improvement-04-6.0041.hdf5\n",
      "37975/37975 [==============================] - 52s 1ms/step - loss: 6.0041\n",
      "Epoch 5/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.9992\n",
      "Epoch 00005: loss improved from 6.00405 to 6.00001, saving model to weights-improvement-05-6.0000.hdf5\n",
      "37975/37975 [==============================] - 58s 2ms/step - loss: 6.0000\n",
      "Epoch 6/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.9923\n",
      "Epoch 00006: loss improved from 6.00001 to 5.99230, saving model to weights-improvement-06-5.9923.hdf5\n",
      "37975/37975 [==============================] - 56s 1ms/step - loss: 5.9923\n",
      "Epoch 7/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.9808\n",
      "Epoch 00007: loss improved from 5.99230 to 5.98003, saving model to weights-improvement-07-5.9800.hdf5\n",
      "37975/37975 [==============================] - 52s 1ms/step - loss: 5.9800\n",
      "Epoch 8/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.9644\n",
      "Epoch 00008: loss improved from 5.98003 to 5.96467, saving model to weights-improvement-08-5.9647.hdf5\n",
      "37975/37975 [==============================] - 58s 2ms/step - loss: 5.9647\n",
      "Epoch 9/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.9415\n",
      "Epoch 00009: loss improved from 5.96467 to 5.94184, saving model to weights-improvement-09-5.9418.hdf5\n",
      "37975/37975 [==============================] - 54s 1ms/step - loss: 5.9418\n",
      "Epoch 10/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.9098\n",
      "Epoch 00010: loss improved from 5.94184 to 5.91067, saving model to weights-improvement-10-5.9107.hdf5\n",
      "37975/37975 [==============================] - 53s 1ms/step - loss: 5.9107\n",
      "Epoch 11/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.8657\n",
      "Epoch 00011: loss improved from 5.91067 to 5.86593, saving model to weights-improvement-11-5.8659.hdf5\n",
      "37975/37975 [==============================] - 58s 2ms/step - loss: 5.8659\n",
      "Epoch 12/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.8106\n",
      "Epoch 00012: loss improved from 5.86593 to 5.80987, saving model to weights-improvement-12-5.8099.hdf5\n",
      "37975/37975 [==============================] - 49s 1ms/step - loss: 5.8099\n",
      "Epoch 13/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.7374\n",
      "Epoch 00013: loss improved from 5.80987 to 5.73673, saving model to weights-improvement-13-5.7367.hdf5\n",
      "37975/37975 [==============================] - 48s 1ms/step - loss: 5.7367\n",
      "Epoch 14/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.6515\n",
      "Epoch 00014: loss improved from 5.73673 to 5.65033, saving model to weights-improvement-14-5.6503.hdf5\n",
      "37975/37975 [==============================] - 46s 1ms/step - loss: 5.6503\n",
      "Epoch 15/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.5491\n",
      "Epoch 00015: loss improved from 5.65033 to 5.54929, saving model to weights-improvement-15-5.5493.hdf5\n",
      "37975/37975 [==============================] - 46s 1ms/step - loss: 5.5493\n",
      "Epoch 16/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.4290\n",
      "Epoch 00016: loss improved from 5.54929 to 5.42995, saving model to weights-improvement-16-5.4300.hdf5\n",
      "37975/37975 [==============================] - 48s 1ms/step - loss: 5.4300\n",
      "Epoch 17/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.3030\n",
      "Epoch 00017: loss improved from 5.42995 to 5.30306, saving model to weights-improvement-17-5.3031.hdf5\n",
      "37975/37975 [==============================] - 49s 1ms/step - loss: 5.3031\n",
      "Epoch 18/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.1609\n",
      "Epoch 00018: loss improved from 5.30306 to 5.16082, saving model to weights-improvement-18-5.1608.hdf5\n",
      "37975/37975 [==============================] - 50s 1ms/step - loss: 5.1608\n",
      "Epoch 19/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 5.0197\n",
      "Epoch 00019: loss improved from 5.16082 to 5.01957, saving model to weights-improvement-19-5.0196.hdf5\n",
      "37975/37975 [==============================] - 47s 1ms/step - loss: 5.0196\n",
      "Epoch 20/20\n",
      "37888/37975 [============================>.] - ETA: 0s - loss: 4.8804\n",
      "Epoch 00020: loss improved from 5.01957 to 4.88047, saving model to weights-improvement-20-4.8805.hdf5\n",
      "37975/37975 [==============================] - 48s 1ms/step - loss: 4.8805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11542e160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_in, data_out, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
