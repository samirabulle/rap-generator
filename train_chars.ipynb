{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samirabulle/Documents/projects/rap_generator/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PyLyrics import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import h5py as h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = open(\"lyrics.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samirabulle/Documents/projects/rap_generator/env/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/samirabulle/.pyenv/versions/3.6.4/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Song or Singer does not exist or the API does not have Lyrics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8409f1a20ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mall_lyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPyLyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'J. Cole'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkendrick_tracks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mall_lyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPyLyrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kendrick Lamar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/projects/rap_generator/env/lib/python3.6/site-packages/PyLyrics/functions.py\u001b[0m in \u001b[0;36mgetLyrics\u001b[0;34m(singer, song)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mlyrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'lyricbox'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlyrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Song or Singer does not exist or the API does not have Lyrics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m#Remove Scripts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Song or Singer does not exist or the API does not have Lyrics"
     ]
    }
   ],
   "source": [
    "drake = \"\"\"Tuscan Leather\n",
    "Furthest Thing\n",
    "Started from the Bottom\n",
    "Wu-Tang Forever\n",
    "Own It\n",
    "Worst Behavior\n",
    "From Time (featuring JhenÃ© Aiko)\n",
    "Hold On, We're Going Home (featuring Majid Jordan)\n",
    "Connect\n",
    "The Language\n",
    "305 to My City (featuring Detail)\n",
    "Too Much (featuring Sampha)\n",
    "Pound Cake / Paris Morton Music 2 (featuring Jay-Z)\n",
    "Come Thru (Deluxe edition bonus track)\n",
    "All Me (featuring 2 Chainz and Big Sean) (Deluxe edition bonus track)\n",
    "The Motion (featuring Sampha) (Best Buy bonus track)\"\"\"\n",
    "\n",
    "j_cole = \"\"\"Intro\n",
    "January 28th\n",
    "Wet Dreamz\n",
    "03' Adolescence\n",
    "A Tale of 2 Citiez\n",
    "Fire Squad\n",
    "St. Tropez\n",
    "G.O.M.D.\n",
    "No Role Modelz\n",
    "Hello\n",
    "Apparently\n",
    "Love Yourz\n",
    "Note to Self\"\"\"\n",
    "\n",
    "kendrick = \"\"\"Sherane a.k.a. Master Splinter's Daughter\n",
    "Bitch, Don't Kill My Vibe\n",
    "Backseat Freestyle\n",
    "The Art of Peer Pressure\n",
    "Money Trees (featuring Jay Rock)\n",
    "Poetic Justice (featuring Drake)\n",
    "good kid\n",
    "m.A.A.d city (featuring MC Eiht)\n",
    "Swimming Pools (Drank) (Extended Version)\n",
    "Sing About Me, I'm Dying of Thirst\n",
    "Real (featuring Anna Wise)\n",
    "Compton (featuring Dr. Dre)\n",
    "The Recipe (featuring Dr. Dre) (Deluxe edition track)\n",
    "Black Boy Fly (Deluxe edition track)\n",
    "Now or Never (featuring Mary J. Blige) (Deluxe edition track)\n",
    "Collect Calls (featuring Kent Jamz) (iTunes deluxe edition track)\n",
    "Swimming Pools (Drank) (iTunes deluxe edition track)\n",
    "County Building Blues (Target bonus track)\"\"\"\n",
    "\n",
    "drake_tracks = drake.split(\"\\n\")\n",
    "j_cole_tracks = j_cole.split(\"\\n\")\n",
    "kendrick_tracks = kendrick.split(\"\\n\")\n",
    "\n",
    "for track in drake_tracks:\n",
    "    all_lyrics.write(PyLyrics.getLyrics('Drake',track) + '\\n')\n",
    "for track in j_cole_tracks:\n",
    "    all_lyrics.write(PyLyrics.getLyrics('J. Cole',track) + '\\n')\n",
    "for track in kendrick_tracks:\n",
    "    all_lyrics.write(PyLyrics.getLyrics('Kendrick Lamar',track) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_lyrics.close()\n",
    "raw_lyrics = open(\"lyrics.txt\",\"r\")\n",
    "raw_lyrics = raw_lyrics.read()\n",
    "raw_lyrics = raw_lyrics.lower()\n",
    "#lyric_tokens = word_tokenize(raw_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samirabulle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_lyrics)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(raw_lyrics)\n",
    "n_vocab = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "data_input = []\n",
    "data_output = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_lyrics[i:i + seq_length]\n",
    "    seq_out = raw_lyrics[i + seq_length]\n",
    "    data_input.append([char_to_int[word] for word in seq_in])\n",
    "    data_output.append(char_to_int[seq_out])\n",
    "n_patterns = len(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = numpy.reshape(data_input, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data_in/float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = np_utils.to_categorical(data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(data_in.shape[1], data_in.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(data_out.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.9355\n",
      "Epoch 00001: loss improved from inf to 2.93546, saving model to weights-improvement-01-2.9355.hdf5\n",
      "165192/165192 [==============================] - 1036s 6ms/step - loss: 2.9355\n",
      "Epoch 2/20\n",
      "165120/165192 [============================>.] - ETA: 1s - loss: 2.7799\n",
      "Epoch 00002: loss improved from 2.93546 to 2.77989, saving model to weights-improvement-02-2.7799.hdf5\n",
      "165192/165192 [==============================] - 3563s 22ms/step - loss: 2.7799\n",
      "Epoch 3/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.7197\n",
      "Epoch 00003: loss improved from 2.77989 to 2.71969, saving model to weights-improvement-03-2.7197.hdf5\n",
      "165192/165192 [==============================] - 960s 6ms/step - loss: 2.7197\n",
      "Epoch 4/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.6684\n",
      "Epoch 00004: loss improved from 2.71969 to 2.66828, saving model to weights-improvement-04-2.6683.hdf5\n",
      "165192/165192 [==============================] - 898s 5ms/step - loss: 2.6683\n",
      "Epoch 5/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.6141\n",
      "Epoch 00005: loss improved from 2.66828 to 2.61402, saving model to weights-improvement-05-2.6140.hdf5\n",
      "165192/165192 [==============================] - 1338s 8ms/step - loss: 2.6140\n",
      "Epoch 6/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.5549\n",
      "Epoch 00006: loss improved from 2.61402 to 2.55485, saving model to weights-improvement-06-2.5549.hdf5\n",
      "165192/165192 [==============================] - 854s 5ms/step - loss: 2.5549\n",
      "Epoch 7/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.4921\n",
      "Epoch 00007: loss improved from 2.55485 to 2.49216, saving model to weights-improvement-07-2.4922.hdf5\n",
      "165192/165192 [==============================] - 847s 5ms/step - loss: 2.4922\n",
      "Epoch 8/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.4306\n",
      "Epoch 00008: loss improved from 2.49216 to 2.43048, saving model to weights-improvement-08-2.4305.hdf5\n",
      "165192/165192 [==============================] - 875s 5ms/step - loss: 2.4305\n",
      "Epoch 9/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.3763\n",
      "Epoch 00009: loss improved from 2.43048 to 2.37630, saving model to weights-improvement-09-2.3763.hdf5\n",
      "165192/165192 [==============================] - 855s 5ms/step - loss: 2.3763\n",
      "Epoch 10/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.3240\n",
      "Epoch 00010: loss improved from 2.37630 to 2.32402, saving model to weights-improvement-10-2.3240.hdf5\n",
      "165192/165192 [==============================] - 910s 6ms/step - loss: 2.3240\n",
      "Epoch 11/20\n",
      "165120/165192 [============================>.] - ETA: 1s - loss: 2.2766\n",
      "Epoch 00011: loss improved from 2.32402 to 2.27648, saving model to weights-improvement-11-2.2765.hdf5\n",
      "165192/165192 [==============================] - 3849s 23ms/step - loss: 2.2765\n",
      "Epoch 12/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.2335\n",
      "Epoch 00012: loss improved from 2.27648 to 2.23363, saving model to weights-improvement-12-2.2336.hdf5\n",
      "165192/165192 [==============================] - 826s 5ms/step - loss: 2.2336\n",
      "Epoch 13/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.2243\n",
      "Epoch 00013: loss improved from 2.23363 to 2.22436, saving model to weights-improvement-13-2.2244.hdf5\n",
      "165192/165192 [==============================] - 834s 5ms/step - loss: 2.2244\n",
      "Epoch 14/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.1584\n",
      "Epoch 00014: loss improved from 2.22436 to 2.15837, saving model to weights-improvement-14-2.1584.hdf5\n",
      "165192/165192 [==============================] - 803s 5ms/step - loss: 2.1584\n",
      "Epoch 15/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.1263\n",
      "Epoch 00015: loss improved from 2.15837 to 2.12624, saving model to weights-improvement-15-2.1262.hdf5\n",
      "165192/165192 [==============================] - 807s 5ms/step - loss: 2.1262\n",
      "Epoch 16/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.0951\n",
      "Epoch 00016: loss improved from 2.12624 to 2.09504, saving model to weights-improvement-16-2.0950.hdf5\n",
      "165192/165192 [==============================] - 1956s 12ms/step - loss: 2.0950\n",
      "Epoch 17/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 2.0734\n",
      "Epoch 00017: loss improved from 2.09504 to 2.07347, saving model to weights-improvement-17-2.0735.hdf5\n",
      "165192/165192 [==============================] - 853s 5ms/step - loss: 2.0735\n",
      "Epoch 18/20\n",
      "165120/165192 [============================>.] - ETA: 15s - loss: 2.0343\n",
      "Epoch 00018: loss improved from 2.07347 to 2.03427, saving model to weights-improvement-18-2.0343.hdf5\n",
      "165192/165192 [==============================] - 35637s 216ms/step - loss: 2.0343\n",
      "Epoch 19/20\n",
      "165120/165192 [============================>.] - ETA: 2s - loss: 2.0138\n",
      "Epoch 00019: loss improved from 2.03427 to 2.01380, saving model to weights-improvement-19-2.0138.hdf5\n",
      "165192/165192 [==============================] - 4966s 30ms/step - loss: 2.0138\n",
      "Epoch 20/20\n",
      "165120/165192 [============================>.] - ETA: 0s - loss: 1.9995\n",
      "Epoch 00020: loss improved from 2.01380 to 1.99948, saving model to weights-improvement-20-1.9995.hdf5\n",
      "165192/165192 [==============================] - 871s 5ms/step - loss: 1.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12530bda0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_in, data_out, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
